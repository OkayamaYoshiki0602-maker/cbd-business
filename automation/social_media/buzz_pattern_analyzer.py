#!/usr/bin/env python3
"""
ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ”¹å–„ç‰ˆï¼‰
ä»–ã‚¸ãƒ£ãƒ³ãƒ«ã®å°‚é–€ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‹ã‚‰ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã®æœ¬è³ªã‚’æŠ½å‡º
"""

import os
import sys
import re
from pathlib import Path
from dotenv import load_dotenv

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', '')


def analyze_buzz_pattern(tweet_text, engagement_metrics=None):
    """
    ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ãƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
    
    Args:
        tweet_text: ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        engagement_metrics: ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ï¼ˆã„ã„ã­æ•°ã€ãƒªãƒ„ã‚¤ãƒ¼ãƒˆæ•°ãªã©ï¼‰
    
    Returns:
        ãƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¾æ›¸
    """
    patterns = {
        'has_emoji': bool(re.search(r'[ğŸ˜€-ğŸ™ğŸŒ€-ğŸ—¿]', tweet_text)),
        'has_number': bool(re.search(r'\d+', tweet_text)),
        'has_date': bool(re.search(r'\d{1,2}æœˆ|\d{1,2}æ—¥|202\d', tweet_text)),
        'has_question': 'ï¼Ÿ' in tweet_text or '?' in tweet_text,
        'has_exclamation': 'ï¼' in tweet_text or '!' in tweet_text,
        'line_breaks': tweet_text.count('\n'),
        'has_title': bool(re.search(r'^ã€|^ã€Œ|^ã€|^â– |^â–¶|^â—', tweet_text)),
        'has_bullet': 'ãƒ»' in tweet_text or 'â€¢' in tweet_text or 'â†’' in tweet_text,
        'has_ellipsis': 'â€¦' in tweet_text or '...' in tweet_text,
        'has_dakuten': bool(re.search(r'[ãŒ-ã½]|[ã‚¬-ãƒ]', tweet_text)),
        'length': len(tweet_text),
        'sentence_count': len(re.split(r'[ã€‚ï¼ï¼Ÿ]', tweet_text)),
    }
    
    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã®æœ‰ç„¡
    patterns['has_hashtag'] = bool(re.search(r'#\w+', tweet_text))
    
    # æ”¹è¡Œãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆæ®µè½ã®æœ‰ç„¡ï¼‰
    lines = tweet_text.split('\n')
    patterns['paragraph_count'] = len([l for l in lines if l.strip()])
    patterns['has_paragraph_break'] = patterns['paragraph_count'] > 2
    
    # æ¿ç‚¹ãƒ»åŠæ¿ç‚¹ã®æ´»ç”¨åº¦ï¼ˆå¼·èª¿ã®æŒ‡æ¨™ï¼‰
    dakuten_chars = len(re.findall(r'[ãŒ-ã½]|[ã‚¬-ãƒ]', tweet_text))
    patterns['dakuten_density'] = dakuten_chars / max(len(tweet_text), 1)
    
    return patterns


def extract_buzz_essence(tweets_data, use_ai=True):
    """
    è¤‡æ•°ã®ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã‹ã‚‰æœ¬è³ªã‚’æŠ½å‡ºï¼ˆAIæ´»ç”¨ï¼‰
    
    Args:
        tweets_data: ãƒ„ã‚¤ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆï¼ˆãƒ†ã‚­ã‚¹ãƒˆã€ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆæŒ‡æ¨™ãªã©ï¼‰
        use_ai: AIã‚’ä½¿ç”¨ã™ã‚‹ã‹
    
    Returns:
        ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã®æœ¬è³ªï¼ˆæ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€è¦ç´ ãªã©ï¼‰
    """
    if not tweets_data:
        return None
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
    all_patterns = []
    for tweet_data in tweets_data[:10]:  # æœ€å¤§10ä»¶
        tweet_text = tweet_data.get('text', '')
        if not tweet_text:
            continue
        
        patterns = analyze_buzz_pattern(tweet_text, tweet_data.get('metrics'))
        patterns['text'] = tweet_text[:100]  # ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦ä¿å­˜
        all_patterns.append(patterns)
    
    if not all_patterns:
        return None
    
    # AIã§æœ¬è³ªã‚’æŠ½å‡º
    if use_ai and GEMINI_API_KEY:
        try:
            import google.generativeai as genai
            
            genai.configure(api_key=GEMINI_API_KEY)
            try:
                model = genai.GenerativeModel('gemini-2.5-flash')
            except:
                try:
                    model = genai.GenerativeModel('gemini-3-flash-preview')
                except:
                    model = genai.GenerativeModel('gemini-2.0-flash')
            
            # ãƒ„ã‚¤ãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒ«ã‚’ã¾ã¨ã‚ã‚‹
            tweet_samples = '\n\n'.join([p['text'] for p in all_patterns[:5]])
            
            prompt = f"""ã‚ãªãŸã¯SNSãƒãƒ¼ã‚±ãƒ†ã‚£ãƒ³ã‚°ã®å°‚é–€å®¶ã§ã™ã€‚

ä»¥ä¸‹ã®ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã‚’åˆ†æã—ã¦ã€ãƒã‚ºã‚‹æœ¬è³ªï¼ˆæ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³ã€è¦ç´ ã€è¡¨ç¾æ–¹æ³•ï¼‰ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

åˆ†æé …ç›®:
1. **æ§‹æˆãƒ‘ã‚¿ãƒ¼ãƒ³**: ãƒ„ã‚¤ãƒ¼ãƒˆã®æ§‹é€ ï¼ˆå°å…¥â†’æœ¬é¡Œâ†’çµè«–ãªã©ï¼‰
2. **è¦–è¦šçš„é­…åŠ›**: æ”¹è¡Œã€è¨˜å·ã€çµµæ–‡å­—ã®ä½¿ã„æ–¹
3. **æƒ…å ±ã®æç¤ºæ–¹æ³•**: æ•°å­—ã€æ—¥ä»˜ã€å…·ä½“çš„ãªæƒ…å ±ã®ä½¿ã„æ–¹
4. **èª­è€…ã®æ„Ÿæƒ…ã«è¨´ãˆã‚‹è¦ç´ **: ç–‘å•ã€é©šãã€å…±æ„Ÿã‚’å‘¼ã¶è¡¨ç¾
5. **æ¿ç‚¹ãƒ»å¼·èª¿ã®æ´»ç”¨**: é‡è¦ãªéƒ¨åˆ†ã®å¼·èª¿æ–¹æ³•

ãƒ„ã‚¤ãƒ¼ãƒˆã‚µãƒ³ãƒ—ãƒ«:
{tweet_samples}

ãƒã‚ºã‚‹æœ¬è³ªã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„ï¼ˆ300æ–‡å­—ä»¥å†…ï¼‰:
"""
            
            response = model.generate_content(prompt)
            essence = response.text.strip()
            
            return {
                'essence': essence,
                'patterns': all_patterns[0],  # ä»£è¡¨çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³
                'common_patterns': _extract_common_patterns(all_patterns)
            }
        
        except Exception as e:
            print(f"âš ï¸ AIåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return _extract_manual_essence(all_patterns)
    
    else:
        return _extract_manual_essence(all_patterns)


def _extract_common_patterns(patterns_list):
    """å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
    if not patterns_list:
        return {}
    
    common = {
        'avg_line_breaks': sum(p['line_breaks'] for p in patterns_list) / len(patterns_list),
        'has_title_rate': sum(1 for p in patterns_list if p['has_title']) / len(patterns_list),
        'has_emoji_rate': sum(1 for p in patterns_list if p['has_emoji']) / len(patterns_list),
        'has_number_rate': sum(1 for p in patterns_list if p['has_number']) / len(patterns_list),
        'avg_length': sum(p['length'] for p in patterns_list) / len(patterns_list),
        'has_hashtag_rate': sum(1 for p in patterns_list if p['has_hashtag']) / len(patterns_list),
    }
    
    return common


def _extract_manual_essence(patterns_list):
    """æ‰‹å‹•ã§æœ¬è³ªã‚’æŠ½å‡ºï¼ˆAIãŒä½¿ãˆãªã„å ´åˆï¼‰"""
    common = _extract_common_patterns(patterns_list)
    
    essence_parts = []
    
    if common['has_title_rate'] > 0.5:
        essence_parts.append("ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆã€ã€‘ã€Œã€â– ãªã©ï¼‰ã‚’ä½¿ç”¨")
    
    if common['avg_line_breaks'] > 2:
        essence_parts.append("æ”¹è¡Œã‚’æ´»ç”¨ã—ãŸæ®µè½æ§‹æˆ")
    
    if common['has_number_rate'] > 0.7:
        essence_parts.append("å…·ä½“çš„ãªæ•°å­—ã‚’å«ã‚€")
    
    if common['has_emoji_rate'] > 0.5:
        essence_parts.append("çµµæ–‡å­—ã‚’åŠ¹æœçš„ã«ä½¿ç”¨")
    
    if common['has_hashtag_rate'] < 0.3:
        essence_parts.append("ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã¯æ§ãˆã‚ã¾ãŸã¯ä¸ä½¿ç”¨")
    
    essence = "ã€‚".join(essence_parts) if essence_parts else "ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
    
    return {
        'essence': essence,
        'patterns': patterns_list[0] if patterns_list else {},
        'common_patterns': common
    }


def apply_buzz_pattern(tweet_text, buzz_essence=None):
    """
    ãƒã‚ºãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ã¦ãƒ„ã‚¤ãƒ¼ãƒˆã‚’æ”¹å–„
    
    Args:
        tweet_text: å…ƒã®ãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
        buzz_essence: ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã®æœ¬è³ª
    
    Returns:
        æ”¹å–„ã•ã‚ŒãŸãƒ„ã‚¤ãƒ¼ãƒˆãƒ†ã‚­ã‚¹ãƒˆ
    """
    if not buzz_essence:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ”¹å–„ã‚’é©ç”¨
        return _improve_tweet_default(tweet_text)
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åŸºã¥ã„ã¦æ”¹å–„
    improved = tweet_text
    
    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’å‰Šé™¤
    improved = re.sub(r'#\w+\s*', '', improved)
    improved = re.sub(r'\s*#\w+$', '', improved)
    
    # æ”¹è¡Œã‚’é©åˆ‡ã«è¿½åŠ 
    if buzz_essence.get('common_patterns', {}).get('avg_line_breaks', 0) > 2:
        # æ®µè½ã‚’æ„è­˜ã—ãŸæ”¹è¡Œ
        improved = _add_paragraph_breaks(improved)
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
    if buzz_essence.get('common_patterns', {}).get('has_title_rate', 0) > 0.5:
        improved = _add_title(improved)
    
    # æ¿ç‚¹ã‚’æ´»ç”¨ã—ãŸå¼·èª¿
    improved = _enhance_with_dakuten(improved)
    
    return improved.strip()


def _improve_tweet_default(tweet_text):
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ”¹å–„ã‚’é©ç”¨"""
    improved = tweet_text
    
    # ãƒãƒƒã‚·ãƒ¥ã‚¿ã‚°ã‚’å‰Šé™¤
    improved = re.sub(r'#\w+\s*', '', improved)
    improved = re.sub(r'\s*#\w+$', '', improved)
    
    # æ”¹è¡Œã‚’é©åˆ‡ã«è¿½åŠ ï¼ˆå¥ç‚¹ã®å¾Œï¼‰
    improved = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*([^ã€‚ï¼ï¼Ÿ\n])', r'\1\n\n\2', improved)
    
    # ä¸è¦ãªç©ºç™½ã‚’æ•´ç†
    improved = re.sub(r'\n{3,}', '\n\n', improved)
    improved = improved.strip()
    
    return improved


def _add_paragraph_breaks(text):
    """æ®µè½ã®æ”¹è¡Œã‚’è¿½åŠ """
    # å¥ç‚¹ã®å¾Œã«æ”¹è¡Œã‚’è¿½åŠ ï¼ˆãŸã ã—é€£ç¶šã—ãªã„ã‚ˆã†ã«ï¼‰
    text = re.sub(r'([ã€‚ï¼ï¼Ÿ])([^\n])', r'\1\n\n\2', text)
    # é€£ç¶šã—ãŸæ”¹è¡Œã‚’2ã¤ã«åˆ¶é™
    text = re.sub(r'\n{3,}', '\n\n', text)
    return text.strip()


def _add_title(text):
    """ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ ï¼ˆå…ˆé ­ã«ï¼‰"""
    # ã™ã§ã«ã‚¿ã‚¤ãƒˆãƒ«ãŒã‚ã‚‹å ´åˆã¯è¿½åŠ ã—ãªã„
    if re.match(r'^ã€|^ã€Œ|^ã€|^â– |^â–¶', text):
        return text
    
    # æœ€åˆã®æ–‡ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã—ã¦æŠ½å‡º
    first_sentence = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', text)[0]
    if len(first_sentence) > 30:
        # ã‚¿ã‚¤ãƒˆãƒ«ãŒé•·ã™ãã‚‹å ´åˆã¯çŸ­ç¸®
        first_sentence = first_sentence[:27] + '...'
    
    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¿½åŠ 
    title = f"ã€{first_sentence}ã€‘"
    remaining = text[len(first_sentence):].strip()
    
    if remaining:
        return f"{title}\n\n{remaining}"
    else:
        return title


def _enhance_with_dakuten(text):
    """æ¿ç‚¹ã‚’æ´»ç”¨ã—ãŸå¼·èª¿ï¼ˆé‡è¦ãªéƒ¨åˆ†ã‚’æ¿ç‚¹ã§å¼·èª¿ï¼‰"""
    # é‡è¦ãã†ãªå˜èªã‚’æ¿ç‚¹ã§å¼·èª¿ï¼ˆä¾‹ï¼šã€Œã¨ã¦ã‚‚ã€â†’ã€Œã©ã¦ã‚‚ã€ãªã©ã¯è‡ªç„¶ã§ã¯ãªã„ã®ã§ã€æ§ãˆã‚ã«ï¼‰
    # å®Ÿéš›ã«ã¯ã€AIç”Ÿæˆæ™‚ã«è‡ªç„¶ãªå¼·èª¿ã‚’è¡Œã†
    return text


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰"""
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿
    test_tweets = [
        {'text': 'ã€æœ€æ–°ç ”ç©¶ã€‘CBDã®åŠ¹æœãŒæ˜ã‚‰ã‹ã«ï¼2024å¹´ã®ãƒ‡ãƒ¼ã‚¿ã§åˆ¤æ˜ã—ãŸé©šãã®äº‹å®Ÿã¨ã¯ï¼Ÿ', 'metrics': {'likes': 1000}},
        {'text': 'å¤§éº»ãƒ“ã‚¸ãƒã‚¹ãŒ1.65å„„ãƒ‰ãƒ«ã‚’è¨˜éŒ²ã€‚ã“ã‚Œã¯ä½•ã‚’æ„å‘³ã™ã‚‹ã®ã‹ï¼Ÿ', 'metrics': {'likes': 800}},
    ]
    
    essence = extract_buzz_essence(test_tweets)
    
    if essence:
        print("ğŸ“Š ãƒã‚ºãƒ„ã‚¤ãƒ¼ãƒˆã®æœ¬è³ª:")
        print("=" * 60)
        print(essence.get('essence', 'åˆ†æã§ãã¾ã›ã‚“ã§ã—ãŸ'))
        print("=" * 60)
        
        print("\nğŸ“ˆ å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³:")
        common = essence.get('common_patterns', {})
        for key, value in common.items():
            print(f"  {key}: {value}")


if __name__ == '__main__':
    main()
