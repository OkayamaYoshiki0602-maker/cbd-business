#!/usr/bin/env python3
"""
DMMå•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒã‚’å–å¾—ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
åŒäººèªŒãƒ»ã‚¢ãƒ€ãƒ«ãƒˆãƒ“ãƒ‡ã‚ªã®ã‚µãƒ³ãƒ—ãƒ«ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
"""

import os
import sys
import re
import requests
from pathlib import Path
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from dotenv import load_dotenv

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

load_dotenv()


class DMMImageExtractor:
    """DMMã‹ã‚‰ç”»åƒã‚’æŠ½å‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.7,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
    
    def extract_images_from_product_page(self, url):
        """
        å•†å“ãƒšãƒ¼ã‚¸ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º
        
        Args:
            url: DMMå•†å“ãƒšãƒ¼ã‚¸ã®URLï¼ˆä¾‹: https://dmm.co.jp/dc/doujin/-/detail/=/cid=d_715045/ï¼‰
        
        Returns:
            list: ç”»åƒURLã®ãƒªã‚¹ãƒˆ
        """
        try:
            print(f"ğŸ“¥ å•†å“ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {url}")
            response = self.session.get(url, timeout=15, allow_redirects=True)
            response.raise_for_status()
            
            # å¹´é½¢ç¢ºèªãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
            if 'age_check' in response.url or 'age-verification' in response.text.lower():
                print("âš ï¸  å¹´é½¢ç¢ºèªãƒšãƒ¼ã‚¸ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                print("ğŸ’¡ æ‰‹å‹•ã§ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹ã—ã€Cookieã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ§˜ã€…ãªæ–¹æ³•ã§ç”»åƒã‚’æ¤œç´¢
            image_urls = []
            
            # 1. ã‚µãƒ³ãƒ—ãƒ«ç”»åƒï¼ˆsampleç”»åƒï¼‰
            sample_images = soup.find_all('img', {'src': re.compile(r'sample|preview|thumbnail', re.I)})
            for img in sample_images:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src and self._is_valid_image_url(src):
                    image_urls.append(urljoin(url, src))
            
            # 2. å•†å“ç”»åƒï¼ˆproductç”»åƒï¼‰
            product_images = soup.find_all('img', {'src': re.compile(r'product|goods|doujin', re.I)})
            for img in product_images:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src and self._is_valid_image_url(src) and src not in image_urls:
                    image_urls.append(urljoin(url, src))
            
            # 3. ä¸€èˆ¬çš„ãªç”»åƒï¼ˆ.jpg, .pngç­‰ï¼‰
            all_images = soup.find_all('img')
            for img in all_images:
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src and self._is_valid_image_url(src):
                    full_url = urljoin(url, src)
                    # ã‚µãƒ³ãƒ—ãƒ«/å•†å“ç”»åƒã®ã¿ï¼ˆãƒ­ã‚´ã‚„åºƒå‘Šã‚’é™¤å¤–ï¼‰
                    if any(keyword in full_url.lower() for keyword in ['sample', 'preview', 'product', 'doujin', 'cid=']):
                        if full_url not in image_urls:
                            image_urls.append(full_url)
            
            # 4. JavaScriptå†…ã®ç”»åƒURLï¼ˆdata-srcå±æ€§ãªã©ï¼‰
            scripts = soup.find_all('script')
            for script in scripts:
                if script.string:
                    # JSONå½¢å¼ã®ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç”»åƒURLã‚’æŠ½å‡º
                    matches = re.findall(r'https?://[^\s"\'<>]+\.(?:jpg|jpeg|png|gif|webp)', script.string, re.I)
                    for match in matches:
                        if any(keyword in match.lower() for keyword in ['sample', 'preview', 'product']):
                            if match not in image_urls:
                                image_urls.append(match)
            
            # é‡è¤‡ã‚’å‰Šé™¤
            image_urls = list(dict.fromkeys(image_urls))  # é †åºã‚’ä¿ã£ãŸã¾ã¾é‡è¤‡å‰Šé™¤
            
            print(f"âœ… {len(image_urls)}å€‹ã®ç”»åƒURLã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            return image_urls
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return []
        except Exception as e:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            import traceback
            traceback.print_exc()
            return []
    
    def _is_valid_image_url(self, url):
        """æœ‰åŠ¹ãªç”»åƒURLã‹ãƒã‚§ãƒƒã‚¯"""
        if not url:
            return False
        if url.startswith('data:'):
            return False
        if any(excluded in url.lower() for excluded in ['logo', 'icon', 'button', 'banner', 'ad']):
            return False
        return any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp'])
    
    def download_image(self, image_url, save_path):
        """
        ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        
        Args:
            image_url: ç”»åƒã®URL
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹
        
        Returns:
            bool: æˆåŠŸã—ãŸã‹ã©ã†ã‹
        """
        try:
            response = self.session.get(image_url, timeout=15, stream=True)
            response.raise_for_status()
            
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            
            # ç”»åƒã‚’ä¿å­˜
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            file_size = os.path.getsize(save_path)
            print(f"âœ… ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ: {save_path} ({file_size} bytes)")
            return True
            
        except Exception as e:
            print(f"âŒ ç”»åƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ ({image_url}): {e}")
            return False
    
    def extract_product_info(self, url):
        """
        å•†å“æƒ…å ±ã‚’æŠ½å‡º
        
        Args:
            url: DMMå•†å“ãƒšãƒ¼ã‚¸ã®URL
        
        Returns:
            dict: å•†å“æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã€ä½œè€…ã€ã‚¸ãƒ£ãƒ³ãƒ«ç­‰ï¼‰
        """
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            info = {
                'title': '',
                'author': '',
                'genre': '',
                'price': '',
                'url': url
            }
            
            # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
            title_selectors = [
                'h1.title',
                '.product-title',
                'h1',
                'title'
            ]
            
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    info['title'] = title_elem.get_text(strip=True)
                    break
            
            # ä½œè€…ã‚’æŠ½å‡º
            author_elem = soup.find('span', string=re.compile(r'ä½œè€…|è‘—è€…|ã‚µãƒ¼ã‚¯ãƒ«', re.I))
            if not author_elem:
                author_elem = soup.find('a', href=re.compile(r'circle|author'))
            if author_elem:
                info['author'] = author_elem.get_text(strip=True)
            
            # ã‚¸ãƒ£ãƒ³ãƒ«ã‚’æŠ½å‡º
            genre_elems = soup.find_all('a', href=re.compile(r'genre|category'))
            if genre_elems:
                info['genre'] = ', '.join([elem.get_text(strip=True) for elem in genre_elems[:3]])
            
            return info
            
        except Exception as e:
            print(f"âŒ å•†å“æƒ…å ±ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            return {'url': url}


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python dmm_image_extractor.py <DMMå•†å“URL> [å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª]")
        print("\nä¾‹:")
        print("  python dmm_image_extractor.py https://dmm.co.jp/dc/doujin/-/detail/=/cid=d_715045/")
        print("  python dmm_image_extractor.py https://dmm.co.jp/dc/doujin/-/detail/=/cid=d_715045/ ./images")
        sys.exit(1)
    
    url = sys.argv[1]
    output_dir = sys.argv[2] if len(sys.argv) > 2 else './dmm_images'
    
    extractor = DMMImageExtractor()
    
    # å•†å“æƒ…å ±ã‚’å–å¾—
    print("\n" + "="*60)
    print("ğŸ“¦ å•†å“æƒ…å ±ã‚’å–å¾—ä¸­...")
    print("="*60)
    product_info = extractor.extract_product_info(url)
    
    print(f"\nğŸ“ å•†å“æƒ…å ±:")
    for key, value in product_info.items():
        print(f"  {key}: {value}")
    
    # ç”»åƒã‚’æŠ½å‡º
    print("\n" + "="*60)
    print("ğŸ–¼ï¸  ç”»åƒã‚’æŠ½å‡ºä¸­...")
    print("="*60)
    image_urls = extractor.extract_images_from_product_page(url)
    
    if not image_urls:
        print("\nâŒ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print("ğŸ’¡ ä»¥ä¸‹ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™:")
        print("  1. å¹´é½¢ç¢ºèªãŒå¿…è¦ãªãƒšãƒ¼ã‚¸ã®å¯èƒ½æ€§")
        print("  2. JavaScriptã§å‹•çš„ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹ç”»åƒã®å¯èƒ½æ€§")
        print("  3. ãƒ­ã‚°ã‚¤ãƒ³ãŒå¿…è¦ãªå¯èƒ½æ€§")
        sys.exit(1)
    
    print(f"\nğŸ“¸ æ¤œå‡ºã•ã‚ŒãŸç”»åƒ ({len(image_urls)}ä»¶):")
    for i, img_url in enumerate(image_urls[:10], 1):  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
        print(f"  {i}. {img_url}")
    
    if len(image_urls) > 10:
        print(f"  ... ä»– {len(image_urls) - 10}ä»¶")
    
    # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®5æšã¾ã§ï¼‰
    print("\n" + "="*60)
    print("ğŸ’¾ ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
    print("="*60)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(output_dir, exist_ok=True)
    
    # å•†å“IDã‚’URLã‹ã‚‰æŠ½å‡ºï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨ï¼‰
    cid_match = re.search(r'cid=([^/]+)', url)
    product_id = cid_match.group(1) if cid_match else 'product'
    
    downloaded_count = 0
    for i, img_url in enumerate(image_urls[:5], 1):  # æœ€åˆã®5æšã®ã¿ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        # æ‹¡å¼µå­ã‚’å–å¾—
        parsed = urlparse(img_url)
        ext = os.path.splitext(parsed.path)[1] or '.jpg'
        filename = f"{product_id}_{i:02d}{ext}"
        save_path = os.path.join(output_dir, filename)
        
        if extractor.download_image(img_url, save_path):
            downloaded_count += 1
    
    print(f"\nâœ… {downloaded_count}æšã®ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸ")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {os.path.abspath(output_dir)}")


if __name__ == '__main__':
    main()
